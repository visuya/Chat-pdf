import sys
import os
import tempfile
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.callbacks.base import BaseCallbackHandler

# ChromaDB ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì •
# Streamlit Cloudì™€ ê°™ì€ í™˜ê²½ì—ì„œ SQLite3 ë²„ì „ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
__import__('pysqlite3')
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

# --- ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ ì •ì˜ ---
# LLM í† í°ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™”ë©´ì— ì¶œë ¥í•˜ê¸° ìœ„í•œ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """LLMì´ ìƒˆ í† í°ì„ ìƒì„±í•  ë•Œë§ˆë‹¤ í˜¸ì¶œë©ë‹ˆë‹¤."""
        self.text += token
        self.container.markdown(self.text) # Markdown í˜•ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ì¶œë ¥

# --- Streamlit UI ì„¤ì • ---
st.set_page_config(page_title="CHATPDF", page_icon="ğŸ“„")
st.title("CHATPDF ğŸ“„")
st.write("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤.")
st.write("---")

# 1. ì‚¬ìš©ì ì…ë ¥ ë°›ê¸° (ì‚¬ì´ë“œë°”)
with st.sidebar:
    st.header("1. ì„¤ì •")
    # OpenAI API í‚¤ ì…ë ¥
    openai_api_key = st.text_input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", type="password")
    st.caption("ğŸ”‘ API í‚¤ëŠ” ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    st.header("2. íŒŒì¼ ì—…ë¡œë“œ")
    # PDF íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.", type="pdf")

# ë©”ì¸ í™”ë©´ êµ¬ì„±
st.header("3. ì§ˆë¬¸í•˜ê¸°")
question = st.text_input("ì—…ë¡œë“œí•œ PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?")

# 'ë‹µë³€ ìƒì„±' ë²„íŠ¼
if st.button("ë‹µë³€ ìƒì„±"):
    # --- ì…ë ¥ê°’ ìœ íš¨ì„± ê²€ì‚¬ ---
    if not uploaded_file:
        st.error("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    elif not openai_api_key:
        st.error("OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif not question:
        st.error("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        # --- ëª¨ë“  ë¡œì§ ì‹¤í–‰ ---
        with st.spinner("PDFë¥¼ ì²˜ë¦¬í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
            try:
                # 2. ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    temp_filepath = tmp_file.name

                # 3. PDF ë¡œë“œ ë° ë¶„í• 
                loader = PyPDFLoader(temp_filepath)
                pages = loader.load_and_split()
                
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=500,  # ì²­í¬ í¬ê¸° ì¦ê°€
                    chunk_overlap=50, # ì˜¤ë²„ë© ì¦ê°€
                    length_function=len,
                    is_separator_regex=False,
                )
                docs = text_splitter.split_documents(pages)

                # 4. ì„ë² ë”© ë° ë²¡í„° DB ìƒì„± (Chroma)
                embeddings = OpenAIEmbeddings(
                    model="text-embedding-3-small",
                    openai_api_key=openai_api_key,
                )
                # from_documentsë¥¼ ì‚¬ìš©í•´ ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³  DBì— ì €ì¥
                db = Chroma.from_documents(docs, embeddings)

                # 5. RAG ì²´ì¸ ì„¤ì • ë° ì‹¤í–‰
                # LLM ì´ˆê¸°í™” (Retrieverìš©ê³¼ Streamingìš©)
                llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, openai_api_key=openai_api_key)
                
                # MultiQueryRetriever ì„¤ì •: í•˜ë‚˜ì˜ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ê´€ì ì—ì„œ ë‹¤ì‹œ ì§ˆë¬¸í•˜ì—¬ ë” ì •í™•í•œ ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
                retriever = MultiQueryRetriever.from_llm(
                    retriever=db.as_retriever(),
                    llm=llm
                )
                
                # í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
                prompt = hub.pull("rlm/rag-prompt")

                # ë‹µë³€ì´ ì¶œë ¥ë  ì˜ì—­
                st.write("---")
                st.subheader("ğŸ¤– AI ë‹µë³€")
                chat_box = st.empty()
                stream_handler = StreamHandler(chat_box)
                
                # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ LLM ì„¤ì •
                streaming_llm = ChatOpenAI(
                    model="gpt-4o-mini",
                    temperature=0,
                    openai_api_key=openai_api_key,
                    streaming=True,
                    callbacks=[stream_handler] # ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬ ì—°ê²°
                )

                # ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…í•˜ëŠ” í•¨ìˆ˜
                def format_docs(docs):
                    return "\n\n".join(doc.page_content for doc in docs)

                # RAG(ê²€ìƒ‰ ì¦ê°• ìƒì„±) ì²´ì¸ êµ¬ì„±
                rag_chain = (
                    {"context": retriever | format_docs, "question": RunnablePassthrough()}
                    | prompt
                    | streaming_llm  # ìŠ¤íŠ¸ë¦¬ë° LLM ì‚¬ìš©
                    | StrOutputParser()
                )

                # ì²´ì¸ ì‹¤í–‰ (ê²°ê³¼ëŠ” ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ê°€ chat_boxì— ë°”ë¡œ ì¶œë ¥)
                response = rag_chain.invoke(question)

            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                if 'temp_filepath' in locals() and os.path.exists(temp_filepath):
                    os.remove(temp_filepath)

